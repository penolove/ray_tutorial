{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c97d7e6-40bc-416f-97d0-8f1e79db8f7e",
   "metadata": {},
   "source": [
    "# Pytorch Lighting with Ray\n",
    "\n",
    "The RayPlugin provides `Distributed Data Parallel` training on a Ray cluster. PyTorch DDP is used as the distributed training protocol, and Ray is used to launch and manage the training worker processes.\n",
    "\n",
    "\n",
    "in this notebook we will reproduce the training in \n",
    "https://colab.research.google.com/github/PyTorchLightning/lightning-tutorials/blob/publication/.notebooks/lightning_examples/mnist-hello-world.ipynb\n",
    " \n",
    "\n",
    "also you can find more detail about DDP in https://pytorch.org/tutorials/intermediate/ddp_tutorial.html\n",
    "\n",
    "source code of ddp implementation in ray: https://docs.ray.io/en/master/_modules/ray_lightning/ray_ddp.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6b9eb28-db5a-4d6b-a818-76b150f8f5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from ray_lightning import RayPlugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e451874b-db32-400f-aeff-1e5923765a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchmetrics import Accuracy\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "PATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adc86cab-94fa-41b3-9738-ab787cf0a838",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LitMNIST(LightningModule):\n",
    "    def __init__(self, data_dir=PATH_DATASETS, hidden_size=64, learning_rate=2e-4):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Set our init args as class attributes\n",
    "        self.data_dir = data_dir\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Hardcode some dataset specific attributes\n",
    "        self.num_classes = 10\n",
    "        self.dims = (1, 28, 28)\n",
    "        channels, width, height = self.dims\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Define PyTorch model\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(channels * width * height, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, self.num_classes),\n",
    "        )\n",
    "\n",
    "        self.accuracy = Accuracy()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.accuracy(preds, y)\n",
    "\n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", self.accuracy, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Here we just reuse the validation_step for testing\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    ####################\n",
    "    # DATA RELATED HOOKS\n",
    "    ####################\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        MNIST(self.data_dir, train=True, download=True)\n",
    "        MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=BATCH_SIZE)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=BATCH_SIZE)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d478332-6ce7-4cc4-8764-75142c3c47c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-16 18:21:34,557\tINFO services.py:1340 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2021-12-16 18:21:34,570\tWARNING services.py:1826 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=5.04gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=176)\u001b[0m initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=180)\u001b[0m initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=178)\u001b[0m initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=179)\u001b[0m initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=177)\u001b[0m initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=181)\u001b[0m initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=175)\u001b[0m initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=182)\u001b[0m initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=176)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=176)\u001b[0m   | Name     | Type       | Params\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=176)\u001b[0m ----------------------------------------\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=176)\u001b[0m 0 | model    | Sequential | 55.1 K\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=176)\u001b[0m 1 | accuracy | Accuracy   | 0     \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=176)\u001b[0m ----------------------------------------\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=176)\u001b[0m 55.1 K    Trainable params\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=176)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=176)\u001b[0m 55.1 K    Total params\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=176)\u001b[0m 0.220     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=176)\u001b[0m /usr/local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=176)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayExecutor pid=176)\u001b[0m /usr/local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=176)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=181)\u001b[0m [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=175)\u001b[0m [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=178)\u001b[0m [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=176)\u001b[0m [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                              \n",
      "Training: -1it [00:00, ?it/s])\u001b[0m \n",
      "Epoch 0:   0%|          | 0/235 [00:00<00:00, 2460.00it/s]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayExecutor pid=180)\u001b[0m [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=182)\u001b[0m [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=179)\u001b[0m [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=177)\u001b[0m [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   9%|▊         | 20/235 [00:01<00:12, 17.30it/s, loss=2.19, v_num=6]\n",
      "Epoch 0:  17%|█▋        | 40/235 [00:02<00:10, 18.46it/s, loss=1.89, v_num=6]\n",
      "Epoch 0:  26%|██▌       | 60/235 [00:03<00:09, 18.19it/s, loss=1.55, v_num=6]\n",
      "Epoch 0:  34%|███▍      | 80/235 [00:05<00:09, 16.12it/s, loss=1.23, v_num=6]\n",
      "Epoch 0:  43%|████▎     | 100/235 [00:06<00:08, 15.11it/s, loss=0.994, v_num=6]\n",
      "Epoch 0:  51%|█████     | 120/235 [00:07<00:07, 15.52it/s, loss=0.834, v_num=6]\n",
      "Epoch 0:  60%|█████▉    | 140/235 [00:08<00:05, 15.99it/s, loss=0.775, v_num=6]\n",
      "Epoch 0:  68%|██████▊   | 160/235 [00:09<00:04, 16.15it/s, loss=0.697, v_num=6]\n",
      "Epoch 0:  77%|███████▋  | 180/235 [00:11<00:03, 16.34it/s, loss=0.602, v_num=6]\n",
      "Epoch 0:  85%|████████▌ | 200/235 [00:12<00:02, 16.50it/s, loss=0.559, v_num=6]\n",
      "Epoch 0:  94%|█████████▎| 220/235 [00:13<00:00, 16.83it/s, loss=0.559, v_num=6]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[Am \n",
      "Validating:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=176)\u001b[0m \n",
      "Validating: 100%|██████████| 20/20 [00:00<00:00, 31.46it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 235/235 [00:13<00:00, 17.00it/s, loss=0.542, v_num=6, val_loss=0.447, val_acc=0.886]\n",
      "Epoch 1:   0%|          | 0/235 [00:00<00:00, 2192.53it/s, loss=0.542, v_num=6, val_loss=0.447, val_acc=0.886]\n",
      "Epoch 1:   9%|▊         | 20/235 [00:01<00:12, 17.72it/s, loss=0.514, v_num=6, val_loss=0.447, val_acc=0.886] \n",
      "Epoch 1:  17%|█▋        | 40/235 [00:02<00:10, 17.80it/s, loss=0.471, v_num=6, val_loss=0.447, val_acc=0.886]\n",
      "Epoch 1:  26%|██▌       | 60/235 [00:03<00:09, 18.68it/s, loss=0.557, v_num=6, val_loss=0.447, val_acc=0.886]\n",
      "Epoch 1:  34%|███▍      | 80/235 [00:04<00:07, 19.57it/s, loss=0.413, v_num=6, val_loss=0.447, val_acc=0.886]\n",
      "Epoch 1:  43%|████▎     | 100/235 [00:05<00:06, 20.00it/s, loss=0.429, v_num=6, val_loss=0.447, val_acc=0.886]\n",
      "Epoch 1:  51%|█████     | 120/235 [00:05<00:05, 20.49it/s, loss=0.456, v_num=6, val_loss=0.447, val_acc=0.886]\n",
      "Epoch 1:  60%|█████▉    | 140/235 [00:06<00:04, 20.69it/s, loss=0.403, v_num=6, val_loss=0.447, val_acc=0.886]\n",
      "Epoch 1:  68%|██████▊   | 160/235 [00:07<00:03, 20.93it/s, loss=0.461, v_num=6, val_loss=0.447, val_acc=0.886]\n",
      "Epoch 1:  77%|███████▋  | 180/235 [00:08<00:02, 21.02it/s, loss=0.4, v_num=6, val_loss=0.447, val_acc=0.886]  \n",
      "Epoch 1:  85%|████████▌ | 200/235 [00:10<00:01, 19.67it/s, loss=0.366, v_num=6, val_loss=0.447, val_acc=0.886]\n",
      "Epoch 1:  94%|█████████▎| 220/235 [00:11<00:00, 19.62it/s, loss=0.366, v_num=6, val_loss=0.447, val_acc=0.886]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[Am \n",
      "Validating:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=176)\u001b[0m \n",
      "Validating: 100%|██████████| 20/20 [00:00<00:00, 20.27it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 235/235 [00:12<00:00, 19.05it/s, loss=0.328, v_num=6, val_loss=0.311, val_acc=0.914]\n",
      "                                                           \u001b[A\n",
      "Epoch 2:   0%|          | 0/235 [00:00<00:00, 1976.58it/s, loss=0.328, v_num=6, val_loss=0.311, val_acc=0.914]\n",
      "Epoch 2:   9%|▊         | 20/235 [00:01<00:11, 19.26it/s, loss=0.325, v_num=6, val_loss=0.311, val_acc=0.914] \n",
      "Epoch 2:  17%|█▋        | 40/235 [00:01<00:09, 20.69it/s, loss=0.36, v_num=6, val_loss=0.311, val_acc=0.914] \n",
      "Epoch 2:  26%|██▌       | 60/235 [00:02<00:08, 20.47it/s, loss=0.345, v_num=6, val_loss=0.311, val_acc=0.914]\n",
      "Epoch 2:  34%|███▍      | 80/235 [00:04<00:07, 19.65it/s, loss=0.389, v_num=6, val_loss=0.311, val_acc=0.914]\n",
      "Epoch 2:  43%|████▎     | 100/235 [00:05<00:07, 17.95it/s, loss=0.296, v_num=6, val_loss=0.311, val_acc=0.914]\n",
      "Epoch 2:  51%|█████     | 120/235 [00:06<00:06, 18.39it/s, loss=0.388, v_num=6, val_loss=0.311, val_acc=0.914]\n",
      "Epoch 2:  60%|█████▉    | 140/235 [00:07<00:05, 18.01it/s, loss=0.312, v_num=6, val_loss=0.311, val_acc=0.914]\n",
      "Epoch 2:  68%|██████▊   | 160/235 [00:08<00:04, 17.96it/s, loss=0.349, v_num=6, val_loss=0.311, val_acc=0.914]\n",
      "Epoch 2:  77%|███████▋  | 180/235 [00:09<00:03, 18.16it/s, loss=0.259, v_num=6, val_loss=0.311, val_acc=0.914]\n",
      "Epoch 2:  85%|████████▌ | 200/235 [00:10<00:01, 18.31it/s, loss=0.311, v_num=6, val_loss=0.311, val_acc=0.914]\n",
      "Epoch 2:  94%|█████████▎| 220/235 [00:11<00:00, 18.92it/s, loss=0.311, v_num=6, val_loss=0.311, val_acc=0.914]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[Am \n",
      "Validating:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=176)\u001b[0m \n",
      "Validating: 100%|██████████| 20/20 [00:00<00:00, 33.26it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 235/235 [00:12<00:00, 19.10it/s, loss=0.357, v_num=6, val_loss=0.263, val_acc=0.923]\n",
      "Epoch 2: 100%|██████████| 235/235 [00:12<00:00, 19.09it/s, loss=0.357, v_num=6, val_loss=0.263, val_acc=0.923]\n"
     ]
    }
   ],
   "source": [
    "plugin = RayPlugin(num_workers=8, num_cpus_per_worker=1)\n",
    "\n",
    "# Don't set ``gpus`` in the ``Trainer``.\n",
    "# The actual number of GPUs is determined by ``num_workers``.\n",
    "\n",
    "model = LitMNIST()\n",
    "trainer = Trainer(\n",
    "    gpus=0,\n",
    "    max_epochs=3,\n",
    "    progress_bar_refresh_rate=20,\n",
    "    plugins=[plugin]  # <- only add this line\n",
    ")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f5310e5-58ce-4214-82a6-6c0be4548d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayExecutor pid=3140)\u001b[0m initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=3146)\u001b[0m initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=3145)\u001b[0m initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=3141)\u001b[0m initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=3147)\u001b[0m initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=3143)\u001b[0m initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=3142)\u001b[0m initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=3144)\u001b[0m initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=3140)\u001b[0m /usr/local/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=3140)\u001b[0m   f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0it [00:00, ?it/s]140)\u001b[0m \n",
      "Testing:  50%|█████     | 20/40 [00:00<00:00, 39.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 0.2514459788799286, 'val_acc': 0.9279999732971191}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 40/40 [00:01<00:00, 35.69it/s]--------------------------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=3140)\u001b[0m DATALOADER:0 TEST RESULTS\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=3140)\u001b[0m {'val_acc': 0.9279999732971191, 'val_loss': 0.2514459788799286}\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=3140)\u001b[0m --------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 40/40 [00:01<00:00, 35.42it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer.test(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
